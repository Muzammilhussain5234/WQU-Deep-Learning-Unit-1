{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e552f1ce-22d6-4b1c-ae79-34085bd9b448",
   "metadata": {},
   "source": [
    "# From Baseline CNN to Deeper Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b29ac",
   "metadata": {},
   "source": [
    "### **1. Introduction: From Baseline CNN to Deeper Architectures**\n",
    "\n",
    "In the previous notebook (NB03), we trained a **baseline CNN** on the Oxford-IIIT Pet dataset and analyzed its performance. While it achieved **~13% accuracy** — slightly above random guessing for 37 breeds — our detailed error analysis revealed key shortcomings:\n",
    "\n",
    "- The network relied heavily on **color cues** (e.g., black animals predicted as Bombay, white animals as Samoyed).  \n",
    "- It struggled with **fine-grained distinctions** between visually similar breeds (e.g., Chihuahua vs. Miniature Pinscher).  \n",
    "- It often confused **cats and dogs**, failing to separate species-level features in some cases.\n",
    "\n",
    "These issues highlight a fundamental limitation: the baseline CNN is **too shallow** to build the hierarchical representations (edges → textures → parts → objects) needed for fine-grained breed classification.\n",
    "\n",
    "\n",
    "**What We’ll Do in NB04**\n",
    "\n",
    "In this notebook, we address these limitations by:\n",
    "\n",
    "- **Introducing deeper architectures** (e.g., LeNet, AlexNet-inspired models) that can capture more complex patterns.  \n",
    "- **Comparing performance and parameter efficiency** across models to see how depth affects generalization.  \n",
    "- Revisiting the concept of **feature hierarchies** to understand *why* deeper networks tend to perform better on visual tasks.\n",
    "\n",
    "By the end of this notebook, we’ll have a clearer understanding of how **model depth** impacts learning and be prepared to explore **transfer learning** and more advanced architectures in future notebooks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310edef9",
   "metadata": {},
   "source": [
    "### **2. Reload Data & Baseline Model**\n",
    "\n",
    "In this notebook, we compare multiple CNN architectures (Baseline, LeNet-like, AlexNet-mini).  \n",
    "To ensure **self-containment**, we **reload the dataset and baseline model weights** here — independent of NB02.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- **Dataset:** Oxford-IIIT Pets (37 breeds), resized to **128×128**, normalized to `[-1,1]` (mean/std ≈ 0.5).\n",
    "- **Splits:** \n",
    "  - Training/Validation: 80/20 split from `trainval`.\n",
    "  - Test: Provided separately.\n",
    "- **Device:** CPU-only.\n",
    "- **Baseline Model:** Reload weights trained in NB02 for direct comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa7f71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2944\n",
      "Validation size: 736\n",
      "Test size: 3669\n",
      "Number of classes: 37\n",
      "Baseline CNN loaded successfully (CPU-only). Ready for comparison.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -----------------------------\n",
    "# 1. CPU-Only Device Setup\n",
    "# -----------------------------\n",
    "device = 'cpu'  # Enforce CPU across all code\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Data Transformations\n",
    "# -----------------------------\n",
    "IMG_SIZE = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # scale to [-1,1]\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Load Train/Val/Test Datasets\n",
    "# -----------------------------\n",
    "data_path = \"./data/\"\n",
    "\n",
    "# Trainval dataset (for split)\n",
    "trainval_dataset = datasets.OxfordIIITPet(\n",
    "    root=data_path,\n",
    "    split=\"trainval\",\n",
    "    target_types=\"category\",\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Test dataset (provided separately)\n",
    "test_dataset = datasets.OxfordIIITPet(\n",
    "    root=data_path,\n",
    "    split=\"test\",\n",
    "    target_types=\"category\",\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Train/Val Split (80/20)\n",
    "# -----------------------------\n",
    "val_ratio = 0.2\n",
    "train_size = int((1 - val_ratio) * len(trainval_dataset))\n",
    "val_size = len(trainval_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    trainval_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # ensure reproducibility\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. DataLoaders\n",
    "# -----------------------------\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Confirm splits\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n",
    "print(f\"Number of classes: {len(trainval_dataset.classes)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Baseline CNN Definition (same as NB02)\n",
    "# -----------------------------\n",
    "class PetCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PetCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)   # 3x128x128 -> 16x128x128\n",
    "        self.pool = nn.MaxPool2d(2, 2)                            # -> 16x64x64\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # -> 32x64x64\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128)                   # -> 128\n",
    "        self.fc2 = nn.Linear(128, 37)                             # -> 37 (classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))   # -> 16x64x64\n",
    "        x = self.pool(F.relu(self.conv2(x)))   # -> 32x32x32\n",
    "        x = x.view(-1, 32 * 32 * 32)           # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate and load baseline weights (map to CPU)\n",
    "baseline_model = PetCNN().to(device)\n",
    "baseline_model.load_state_dict(torch.load(\"petcnn_best.pth\", map_location=device, weights_only=False))\n",
    "\n",
    "print(\"Baseline CNN loaded successfully (CPU-only). Ready for comparison.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f5b13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ab4ac-f5b8-405b-b642-37dc1aae04ab",
   "metadata": {},
   "source": [
    "**✅ ALERT: You are now ready to answer the Multiple Choice Questions for this section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bb122-8341-486b-9a5b-dd3d98bcbb88",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3928c9-a081-4d66-8f55-18d019c9998c",
   "metadata": {},
   "source": [
    "**✅ Code Task 5.4.2.1: Create a Small Train/Val Split & DataLoaders**\n",
    "\n",
    "📘 Instruction</br>\n",
    "For quick experiments on CPU, let’s create a small custom split from trainval_dataset and build DataLoaders.\n",
    "\n",
    "- Use random_split with a fixed seed for reproducibility.\n",
    "- Make the split sizes student-chosen (e.g., ~800 for train, ~200 for val), but fill in the ....\n",
    "- Build DataLoaders with batch_size=16 and shuffle=True for train.\n",
    "- Print the sizes and store one batch’s tensor shape in CT_batch_shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f70da688-3884-451c-ac80-705e64f76739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT_train/val sizes: 2944 736\n",
      "CT_batch_shape: torch.Size([16, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# small custom split for quick CPU experiments\n",
    "CT_total = len(trainval_dataset)\n",
    "CT_val_ratio=0.2\n",
    "CT_train_size =int((1-CT_val_ratio)*CT_total)\n",
    "CT_val_size =CT_total-CT_train_size\n",
    "CT_train_dataset, CT_val_dataset = random_split(\n",
    "    trainval_dataset,\n",
    "    [CT_train_size, CT_val_size],\n",
    "    generator=torch.Generator().manual_seed(123)\n",
    ")\n",
    "\n",
    "CT_BATCH_SIZE = 16\n",
    "CT_train_loader = DataLoader(CT_train_dataset, batch_size=CT_BATCH_SIZE, shuffle=...)\n",
    "CT_val_loader   = DataLoader(CT_val_dataset,   batch_size=CT_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Peek one batch and record its shape (N, C, H, W)\n",
    "CT_images, CT_labels = next(iter(CT_train_loader))\n",
    "CT_batch_shape = CT_images.shape\n",
    "print(\"CT_train/val sizes:\", len(CT_train_dataset), len(CT_val_dataset))\n",
    "print(\"CT_batch_shape:\", CT_batch_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f21ce7-fe97-4c57-9ecc-4c8d4ffd2209",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "662327ad-f55a-4917-8f17-871eee8d447c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"450\"\n",
       "            src=\"https://player.vimeo.com/video/1112916483?h=3298dbabb7\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x7f06c74cbb90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "# Bigger video\n",
    "VimeoVideo(\"1112916483\", h=\"3298dbabb7\", width=700, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5cc89c",
   "metadata": {},
   "source": [
    "### **3. Architectural Variants Overview (Concept)**\n",
    "\n",
    "In this notebook, we **compare architectures** to explore how **depth and design choices** affect performance and parameter efficiency.\n",
    "\n",
    "**Architectures We’ll Compare**\n",
    "\n",
    "1. **Baseline CNN (NB02)**  \n",
    "   - 2 convolution layers (3×3) + 2 fully connected layers.  \n",
    "   - Designed for teaching fundamentals; very shallow.\n",
    "\n",
    "2. **LeNet-like CNN (1998)**  \n",
    "   - Classic architecture for MNIST digit recognition.  \n",
    "   - 3 convolution layers + 2 fully connected layers.  \n",
    "   - Slightly deeper; captures more **mid-level features** (edges → textures).\n",
    "\n",
    "3. **AlexNet-mini (2012) / VGG-lite**  \n",
    "   - Inspired by ImageNet-winning networks.  \n",
    "   - 4–5 convolution layers + deeper FC layers.  \n",
    "   - Captures **hierarchical features** (edges → textures → object parts → objects).\n",
    "\n",
    "**High-Level ASCII Diagrams**\n",
    "\n",
    "**(a) Baseline CNN (NB02)**\n",
    "\n",
    "```text\n",
    "Input (3×128×128)\n",
    "│\n",
    "├── Conv(3→16) → ReLU → MaxPool(2×2)\n",
    "│\n",
    "├── Conv(16→32) → ReLU → MaxPool(2×2)\n",
    "│\n",
    "├── Flatten → FC(32×32×32 → 128) → ReLU\n",
    "│\n",
    "└── FC(128 → 37) → Softmax\n",
    "```\n",
    "\n",
    "**(b) LeNet-like CNN**\n",
    "\n",
    "```text\n",
    "Input (3×128×128)\n",
    "│\n",
    "├── Conv(3→16) → ReLU → MaxPool(2×2)\n",
    "│\n",
    "├── Conv(16→32) → ReLU → MaxPool(2×2)\n",
    "│\n",
    "├── Conv(32→64) → ReLU → MaxPool(2×2)\n",
    "│\n",
    "├── Flatten → FC(64×16×16 → 256) → ReLU\n",
    "│\n",
    "└── FC(256 → 37) → Softmax\n",
    "```\n",
    "\n",
    "**(c) AlexNet-mini / VGG-lite**\n",
    "\n",
    "```text\n",
    "Input (3×128×128)\n",
    "│\n",
    "├── Conv(3→32) → ReLU → Conv(32→32) → ReLU → MaxPool(2×2)\n",
    "│\n",
    "├── Conv(32→64) → ReLU → Conv(64→64) → ReLU → MaxPool(2×2)\n",
    "│\n",
    "├── Conv(64→128) → ReLU → MaxPool(2×2)\n",
    "│\n",
    "├── Flatten → FC(128×16×16 → 512) → ReLU → Dropout\n",
    "│\n",
    "└── FC(512 → 37) → Softmax\n",
    "\n",
    "```\n",
    "\n",
    "**Parameter Count and Shape Transformations**\n",
    "\n",
    "| Layer Type    | Baseline CNN                  | LeNet-like           | AlexNet-mini          |\n",
    "|---------------|-------------------------------|----------------------|-----------------------|\n",
    "| **Conv Layers** | Few params ($3\\times3$ filters) | More filters (16→64) | Many filters (32→128) |\n",
    "| **FC Layers**   | Dominant ($32k \\to 128$)       | Moderate ($64\\times16\\times16 \\to 256$) | Larger ($128\\times16\\times16 \\to 512$) |\n",
    "| **Total Params**| ~ 4$M                      | ~ 6$M            | ~ 10$–$12$M       |\n",
    "\n",
    "**Receptive Field Expansion**\n",
    "\n",
    "- **Baseline CNN**: Each neuron “sees” only small local patches.\n",
    "- **LeNet-like**: Deeper layers combine local features → larger effective receptive field.\n",
    "- **AlexNet-mini**: Stacks more conv layers; receptive field covers most of the image, capturing global context (e.g., full face of a dog).\n",
    "\n",
    "\n",
    "**Why Compare These?**\n",
    "\n",
    "- **Highlights trade-offs:**\n",
    "  - Depth vs. computation (more convs = more params but richer features).\n",
    "  - Pooling strategy (aggressive vs gradual downsampling).\n",
    "  - Fully connected size (impact on parameter efficiency).\n",
    "\n",
    "- **Prepares us for Project 6:**\n",
    "  - Introduction to transfer learning (pretrained deeper networks like **ResNet**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e6263",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d57639",
   "metadata": {},
   "source": [
    "### **4. Implementing Architectural Variants (Code)**\n",
    "\n",
    "We now translate the three conceptual architectures into PyTorch models:\n",
    "\n",
    "1. **Baseline CNN** (from NB02)  \n",
    "   - 2 convolution layers → pooling → 2 fully connected layers.\n",
    "\n",
    "2. **LeNet-like CNN** (slightly deeper)  \n",
    "   - 3 convolution layers → pooling after first two → 2 fully connected layers.  \n",
    "   - Inspired by **LeNet-5 (1998)** but adapted to RGB and 128×128 inputs.\n",
    "\n",
    "3. **AlexNet-mini (VGG-lite)**  \n",
    "   - 5 convolution layers grouped in blocks (Conv → ReLU → Conv → ReLU → Pool).  \n",
    "   - Deeper feature hierarchy; mimics **AlexNet/VGG-style** stacking but scaled down for Pets dataset.\n",
    "\n",
    "We keep implementation **consistent**:\n",
    "- Use `nn.Conv2d`, `nn.ReLU`, `nn.MaxPool2d` for feature extraction.\n",
    "- Use `nn.Linear` for classification layers.\n",
    "- Forward pass includes **comments on shape changes** (C×H×W).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "800ddbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Baseline CNN (for reference)\n",
    "# -------------------------------\n",
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)   # 3x128x128 -> 16x128x128\n",
    "        self.pool = nn.MaxPool2d(2, 2)                            # Halve H,W\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # 16x64x64 -> 32x64x64\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128)                   # 32x32x32 -> 128\n",
    "        self.fc2 = nn.Linear(128, 37)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> 16x64x64\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> 32x32x32\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# -------------------------------\n",
    "# 2. LeNet-like CNN (3 conv layers)\n",
    "# -------------------------------\n",
    "class LeNetLikeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetLikeCNN, self).__init__()\n",
    "        # Conv Layer 1: 3 → 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)    # 3x128x128 -> 16x128x128\n",
    "        # Conv Layer 2: 16 → 32\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)   # 16x64x64 -> 32x64x64\n",
    "        # Conv Layer 3: 32 → 64\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)   # 32x32x32 -> 64x32x32\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # After 3 poolings: 128 -> 64 -> 32 -> 16\n",
    "        # Final feature map: 64 x 16 x 16 = 16384\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 256)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(256, 37)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))   # -> 16x64x64\n",
    "        x = self.pool(F.relu(self.conv2(x)))   # -> 32x32x32\n",
    "        x = self.pool(F.relu(self.conv3(x)))   # -> 64x16x16\n",
    "        x = x.view(x.size(0), -1)              # Flatten: 64*16*16 = 16384\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# -------------------------------\n",
    "# 3. AlexNet-mini CNN (5 conv layers)\n",
    "# -------------------------------\n",
    "class AlexNetMini(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNetMini, self).__init__()\n",
    "        # Conv Layer 1: 3 → 32\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)     # 3x128x128 -> 32x128x128\n",
    "        # Conv Layer 2: 32 → 64\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)    # 32x64x64 -> 64x64x64\n",
    "        # Conv Layer 3: 64 → 128\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)   # 64x32x32 -> 128x32x32\n",
    "        # Conv Layer 4: 128 → 128\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)  # 128x16x16 -> 128x16x16\n",
    "        # Conv Layer 5: 128 → 256\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)  # 128x8x8 -> 256x8x8\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # After 5 conv + 5 pool layers:\n",
    "        # 128 -> 64 -> 32 -> 16 -> 8 -> 4 (but here last pooling is applied after conv5)\n",
    "        # Final feature map: 256 x 4 x 4 = 4096\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 37)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))   # -> 32x64x64\n",
    "        x = self.pool(F.relu(self.conv2(x)))   # -> 64x32x32\n",
    "        x = self.pool(F.relu(self.conv3(x)))   # -> 128x16x16\n",
    "        x = self.pool(F.relu(self.conv4(x)))   # -> 128x8x8\n",
    "        x = self.pool(F.relu(self.conv5(x)))   # -> 256x4x4\n",
    "        x = x.view(x.size(0), -1)              # Flatten: 256*4*4 = 4096\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbae707",
   "metadata": {},
   "source": [
    "In the following section, we will:\n",
    "- **Instantiate each model**.\n",
    "- Compute and compare **parameter counts** (baseline vs LeNet-like vs AlexNet-mini).\n",
    "- Discuss trade-offs: **depth vs parameter efficiency vs performance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8445a88c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e855006-865c-4296-b38d-94204feab704",
   "metadata": {},
   "source": [
    "**✅ ALERT: You are now ready to answer the Multiple Choice Questions for this section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73447f92-1cbe-4124-8526-4daf2677d6a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3eb055-9ccb-4afa-927d-9f97dc466be0",
   "metadata": {},
   "source": [
    "**✅ Code Task 5.4.4.1: Implement a LeNet Variant (with Dropout/BatchNorm)**\n",
    "\n",
    "📘 Instruction</br>\n",
    "Implement CT_LeNetVariant as a LeNet-like model with 3 conv blocks, BatchNorm after each conv, and Dropout before the final classifier. Use this spec:\n",
    "- conv1: 3→16, conv2: 16→32, conv3: 32→64, each kernel_size=3, padding=1, followed by ReLU, BatchNorm2d, and MaxPool2d(2).\n",
    "- After three pools, spatial dims: 128 → 64 → 32 → 16, so flatten size: 64*16*16.\n",
    "- fc1: 64*16*16 → 256 (ReLU), Dropout(p=0.5), fc2: 256 → 37.</br>\n",
    "Instantiate as CT_model = CT_LeNetVariant().to(device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d78c3e9-8906-4bbf-a69d-8bb9383c7631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT_LeNetVariant(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=16384, out_features=256, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=37, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CT_LeNetVariant(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(16)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.fc1   = nn.Linear(64*16*16, 256)   # 64*16*16 -> 256\n",
    "        self.drop  = nn.Dropout(p=0.5)\n",
    "        self.fc2   = nn.Linear(256,37)   # 256 -> 37\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "CT_model = CT_LeNetVariant().to(device)\n",
    "print(CT_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbb69deb-6576-413d-b01a-0e4d187298b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT_LeNetVariant(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=16384, out_features=256, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=37, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CT_LeNetVariant(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Conv block 1: 3 -> 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(16)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Conv block 2: 16 -> 32\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Conv block 3: 32 -> 64\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(64)\n",
    "\n",
    "        # After 3 pools: 128 -> 64 -> 32 -> 16, so flatten = 64*16*16\n",
    "        self.fc1   = nn.Linear(64 * 16 * 16, 256)\n",
    "        self.drop  = nn.Dropout(p=0.5)\n",
    "        self.fc2   = nn.Linear(256, 37)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # -> 16x64x64\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # -> 32x32x32\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # -> 64x16x16\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)  # logits (N, 37)\n",
    "        return x\n",
    "\n",
    "CT_model = CT_LeNetVariant().to(device)\n",
    "print(CT_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4841956-4367-4a69-9909-c318c2e075d2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd59689-4785-48c0-9226-435beaaa7ba7",
   "metadata": {},
   "source": [
    "**✅ Code Task 5.4.4.2: Param Count & Forward-Shape Sanity Check**\n",
    "\n",
    "📘 Instruction</br>\n",
    "Write a helper to count parameters and do a single forward pass on one batch from CT_train_loader to verify output shape (N, 37).\n",
    "Store:\n",
    "- CT_n_params (int): total learnable parameters of CT_model\n",
    "- CT_logits (tensor): output logits from a single batch\n",
    "- Print both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d36b0abf-ebfb-4bd2-baa7-3d4462d238be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT_logits shape: tensor([[ 0.0158,  0.0248, -0.0261,  0.0263,  0.0234, -0.0111, -0.0249, -0.0235,\n",
      "          0.0244,  0.0252,  0.0146, -0.0394,  0.0293, -0.0379,  0.0266, -0.0076,\n",
      "          0.0059,  0.0007, -0.0476,  0.0352, -0.0721, -0.0203,  0.0488, -0.0441,\n",
      "         -0.0314, -0.0217, -0.0216,  0.0431,  0.0313, -0.0662, -0.0389, -0.0231,\n",
      "         -0.0688, -0.0114,  0.0012,  0.0080,  0.0258],\n",
      "        [ 0.0099,  0.0212, -0.0249,  0.0378,  0.0173, -0.0198, -0.0272, -0.0195,\n",
      "          0.0384,  0.0215,  0.0155, -0.0538,  0.0273, -0.0480,  0.0384,  0.0011,\n",
      "          0.0020,  0.0062, -0.0576,  0.0327, -0.0737, -0.0217,  0.0591, -0.0535,\n",
      "         -0.0314, -0.0147, -0.0116,  0.0451,  0.0260, -0.0694, -0.0514, -0.0251,\n",
      "         -0.0674, -0.0050, -0.0164,  0.0078,  0.0222],\n",
      "        [ 0.0175,  0.0246, -0.0257,  0.0346,  0.0241, -0.0129, -0.0294, -0.0177,\n",
      "          0.0241,  0.0216,  0.0091, -0.0477,  0.0167, -0.0318,  0.0230, -0.0100,\n",
      "          0.0021,  0.0126, -0.0492,  0.0308, -0.0655, -0.0171,  0.0503, -0.0267,\n",
      "         -0.0267, -0.0269, -0.0197,  0.0372,  0.0217, -0.0674, -0.0429, -0.0134,\n",
      "         -0.0761, -0.0120, -0.0058,  0.0043,  0.0187],\n",
      "        [ 0.0085,  0.0163, -0.0284,  0.0348,  0.0177, -0.0102, -0.0230, -0.0192,\n",
      "          0.0207,  0.0266,  0.0192, -0.0469,  0.0233, -0.0374,  0.0253, -0.0078,\n",
      "         -0.0073,  0.0101, -0.0479,  0.0345, -0.0688, -0.0157,  0.0479, -0.0531,\n",
      "         -0.0188, -0.0153, -0.0253,  0.0387,  0.0324, -0.0693, -0.0646, -0.0142,\n",
      "         -0.0780,  0.0033, -0.0064,  0.0106,  0.0262],\n",
      "        [ 0.0105,  0.0325, -0.0233,  0.0206,  0.0247, -0.0197, -0.0233, -0.0174,\n",
      "          0.0200,  0.0286,  0.0199, -0.0548,  0.0130, -0.0453,  0.0343,  0.0015,\n",
      "          0.0130,  0.0085, -0.0559,  0.0363, -0.0748, -0.0111,  0.0434, -0.0433,\n",
      "         -0.0230, -0.0392, -0.0395,  0.0394,  0.0202, -0.0689, -0.0442, -0.0209,\n",
      "         -0.0746, -0.0082,  0.0032, -0.0022,  0.0373],\n",
      "        [ 0.0120,  0.0198, -0.0167,  0.0332,  0.0218, -0.0095, -0.0224, -0.0157,\n",
      "          0.0314,  0.0232,  0.0064, -0.0497,  0.0307, -0.0418,  0.0292, -0.0090,\n",
      "          0.0016,  0.0080, -0.0421,  0.0238, -0.0697, -0.0147,  0.0431, -0.0392,\n",
      "         -0.0229, -0.0091, -0.0191,  0.0378,  0.0292, -0.0679, -0.0480, -0.0170,\n",
      "         -0.0835, -0.0030,  0.0056,  0.0095,  0.0308],\n",
      "        [ 0.0073,  0.0258, -0.0098,  0.0213,  0.0245, -0.0223, -0.0272, -0.0103,\n",
      "          0.0326,  0.0279,  0.0170, -0.0634,  0.0150, -0.0419,  0.0262, -0.0042,\n",
      "          0.0146,  0.0090, -0.0532,  0.0212, -0.0695, -0.0143,  0.0449, -0.0305,\n",
      "         -0.0295, -0.0224, -0.0239,  0.0314,  0.0163, -0.0710, -0.0365, -0.0197,\n",
      "         -0.0747, -0.0064, -0.0120,  0.0082,  0.0353],\n",
      "        [ 0.0044,  0.0246, -0.0251,  0.0298,  0.0281, -0.0163, -0.0289, -0.0179,\n",
      "          0.0318,  0.0263,  0.0173, -0.0537,  0.0187, -0.0354,  0.0290, -0.0084,\n",
      "          0.0106,  0.0077, -0.0504,  0.0242, -0.0670, -0.0035,  0.0448, -0.0406,\n",
      "         -0.0297, -0.0131, -0.0248,  0.0422,  0.0261, -0.0713, -0.0469, -0.0202,\n",
      "         -0.0730, -0.0074, -0.0181,  0.0079,  0.0324],\n",
      "        [ 0.0141,  0.0282, -0.0154,  0.0288,  0.0229, -0.0156, -0.0215, -0.0112,\n",
      "          0.0291,  0.0193,  0.0137, -0.0583,  0.0175, -0.0470,  0.0351, -0.0080,\n",
      "          0.0049,  0.0057, -0.0532,  0.0359, -0.0758, -0.0182,  0.0379, -0.0353,\n",
      "         -0.0261, -0.0202, -0.0268,  0.0314,  0.0229, -0.0696, -0.0388, -0.0317,\n",
      "         -0.0694, -0.0046,  0.0025, -0.0032,  0.0313],\n",
      "        [-0.0006,  0.0211, -0.0181,  0.0300,  0.0166, -0.0072, -0.0188, -0.0234,\n",
      "          0.0246,  0.0280,  0.0263, -0.0535,  0.0275, -0.0424,  0.0218, -0.0128,\n",
      "          0.0062,  0.0081, -0.0449,  0.0342, -0.0772, -0.0073,  0.0577, -0.0418,\n",
      "         -0.0188, -0.0382, -0.0278,  0.0308,  0.0234, -0.0623, -0.0532, -0.0136,\n",
      "         -0.0740,  0.0003, -0.0104,  0.0039,  0.0303],\n",
      "        [ 0.0180,  0.0314, -0.0094,  0.0236,  0.0310, -0.0214, -0.0297, -0.0103,\n",
      "          0.0347,  0.0227,  0.0196, -0.0573,  0.0138, -0.0368,  0.0334, -0.0065,\n",
      "          0.0109,  0.0163, -0.0531,  0.0246, -0.0639, -0.0111,  0.0440, -0.0397,\n",
      "         -0.0338, -0.0213, -0.0271,  0.0266,  0.0270, -0.0698, -0.0379, -0.0160,\n",
      "         -0.0748, -0.0128, -0.0138,  0.0073,  0.0300],\n",
      "        [-0.0026,  0.0341, -0.0270,  0.0163,  0.0258, -0.0145, -0.0172, -0.0199,\n",
      "          0.0219,  0.0261,  0.0166, -0.0527,  0.0107, -0.0524,  0.0359, -0.0036,\n",
      "          0.0106,  0.0022, -0.0527,  0.0257, -0.0708, -0.0145,  0.0436, -0.0360,\n",
      "         -0.0194, -0.0424, -0.0298,  0.0298,  0.0280, -0.0772, -0.0486, -0.0217,\n",
      "         -0.0782, -0.0095, -0.0058,  0.0067,  0.0450],\n",
      "        [ 0.0091,  0.0264, -0.0317,  0.0325,  0.0318, -0.0122, -0.0179, -0.0329,\n",
      "          0.0250,  0.0234,  0.0157, -0.0487,  0.0171, -0.0482,  0.0290, -0.0079,\n",
      "         -0.0028,  0.0083, -0.0489,  0.0442, -0.0649, -0.0305,  0.0527, -0.0508,\n",
      "         -0.0103, -0.0317, -0.0229,  0.0411,  0.0288, -0.0763, -0.0597, -0.0222,\n",
      "         -0.0821,  0.0016, -0.0140,  0.0043,  0.0370],\n",
      "        [ 0.0109,  0.0317, -0.0239,  0.0264,  0.0247, -0.0095, -0.0221, -0.0129,\n",
      "          0.0240,  0.0273,  0.0113, -0.0426,  0.0346, -0.0395,  0.0336, -0.0016,\n",
      "          0.0008,  0.0163, -0.0617,  0.0328, -0.0681, -0.0180,  0.0526, -0.0452,\n",
      "         -0.0136, -0.0235, -0.0343,  0.0340,  0.0317, -0.0690, -0.0595, -0.0147,\n",
      "         -0.0841, -0.0028, -0.0072,  0.0046,  0.0312],\n",
      "        [ 0.0024,  0.0356, -0.0306,  0.0336,  0.0168, -0.0147, -0.0228, -0.0131,\n",
      "          0.0360,  0.0306,  0.0224, -0.0542,  0.0306, -0.0506,  0.0312,  0.0018,\n",
      "          0.0050, -0.0038, -0.0422,  0.0217, -0.0703, -0.0203,  0.0492, -0.0521,\n",
      "         -0.0129, -0.0218, -0.0226,  0.0421,  0.0257, -0.0722, -0.0624, -0.0150,\n",
      "         -0.0713, -0.0039, -0.0025,  0.0135,  0.0323],\n",
      "        [ 0.0056,  0.0404, -0.0272,  0.0123,  0.0320, -0.0243, -0.0224, -0.0136,\n",
      "          0.0235,  0.0253,  0.0157, -0.0467,  0.0195, -0.0481,  0.0296, -0.0024,\n",
      "          0.0129,  0.0096, -0.0596,  0.0425, -0.0662, -0.0066,  0.0478, -0.0539,\n",
      "         -0.0169, -0.0385, -0.0283,  0.0347,  0.0191, -0.0796, -0.0365, -0.0265,\n",
      "         -0.0819, -0.0003, -0.0084,  0.0063,  0.0445]])\n",
      "CT_n_params: 4227877\n"
     ]
    }
   ],
   "source": [
    "def CT_count_params(model):\n",
    "   \n",
    "    CT_params = sum(i.numel() for i in model.parameters() if i.requires_grad) #numel creates album of entity\n",
    "    return CT_params\n",
    "\n",
    "CT_model.eval()\n",
    "with torch.no_grad():\n",
    "    CT_x, CT_y = next(iter(CT_train_loader))\n",
    "    CT_x = CT_x.to(device)\n",
    "    CT_logits = CT_model(CT_x)\n",
    "    CT_n_params = CT_count_params(CT_model)\n",
    "\n",
    "print(\"CT_logits shape:\", CT_logits)\n",
    "print(\"CT_n_params:\", CT_n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa579b-dd1f-494b-b8e6-a91b1f8d7bb8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820edccc",
   "metadata": {},
   "source": [
    "### **5. Training Strategy**\n",
    "\n",
    "We now train and compare **three CNN architectures** of increasing depth:\n",
    "\n",
    "1. **Baseline CNN** – 2 convolution layers (from NB02).\n",
    "2. **LeNet-like CNN** – 3 convolution layers (moderate depth).\n",
    "3. **AlexNet-mini (VGG-lite)** – 5 convolution layers (deepest).\n",
    "\n",
    "**Fair Comparison Setup**\n",
    "\n",
    "To ensure results are comparable:\n",
    "\n",
    "- **Dataset:** Oxford-IIIT Pets (128×128, normalized to `[-1, 1]`)\n",
    "- **Hardware:** CPU-only\n",
    "- **Hyperparameters (same for all models):**\n",
    "  - Optimizer: `Adam` (learning rate = 0.001)\n",
    "  - Loss: `CrossEntropyLoss`\n",
    "  - Batch size: 32 \n",
    "  - Epochs: 5\n",
    "\n",
    "**Parameter Awareness**\n",
    "\n",
    "- We print **parameter counts** for each model *before training*.\n",
    "- This helps relate **model depth vs parameter count vs performance** later.\n",
    "- Expect:\n",
    "  - **Baseline**: small, fast to train.\n",
    "  - **LeNet-like**: slightly deeper, similar parameter count.\n",
    "  - **AlexNet-mini**: deepest, more capacity but also risk of overfitting.\n",
    "\n",
    "**Training Process**\n",
    "\n",
    "- Reuse modular `train_model()` function from NB02.\n",
    "- Train models sequentially (Baseline → LeNet-like → AlexNet-mini).\n",
    "- Save **best model weights** for each:\n",
    "  - `baseline_best.pth`\n",
    "  - `lenet_best.pth`\n",
    "  - `alexmini_best.pth`\n",
    "\n",
    "Now let’s show how we'd run the training and record parameter counts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce7ff6-db55-44e6-a522-8289622a1d6a",
   "metadata": {},
   "source": [
    "#**DISCLAIMER**: We are simulating the training process and using from now on the pre-trained models that you can see in the notebook files. This means that although you have the code we've used to train the model, we are not running the training loops here; otherwise, the VM would collapse and, more importantly, we wouldn't be able to provide this completely free lab for thousands of students. However, you can try to train the model on other platforms, such as Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ccc7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Accuracy calculation helper\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    return (preds == labels).sum().item() / len(labels)\n",
    "\n",
    "# Training function (CPU-only)\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=5, device='cpu'):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Train and validation phases\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                loader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                loader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            for inputs, labels in loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += (outputs.argmax(1) == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / total_samples\n",
    "            epoch_acc = running_corrects / total_samples\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "                train_accuracies.append(epoch_acc)\n",
    "            else:\n",
    "                val_losses.append(epoch_loss)\n",
    "                val_accuracies.append(epoch_acc)\n",
    "\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"Training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s\")\n",
    "    print(f\"Best val loss: {best_loss:.4f}\")\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, (train_losses, val_losses, train_accuracies, val_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0401e13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineCNN parameters: 4,204,293\n",
      "LeNetLikeCNN parameters: 4,227,653\n",
      "AlexNetMini parameters: 2,652,645\n",
      "\n",
      "= Training Baseline CNN (we are using pre-trainned models) =\n",
      "\n",
      "= Training LeNet-like CNN (we are using pre-trainned models) =\n",
      "\n",
      "= Training AlexNet-mini CNN (we are using pre-trainned models) =\n"
     ]
    }
   ],
   "source": [
    "# parameter count\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Instantiate models\n",
    "baseline_model = BaselineCNN()\n",
    "lenet_model = LeNetLikeCNN()\n",
    "alexmini_model = AlexNetMini()\n",
    "\n",
    "# Print parameter counts\n",
    "print(f\"BaselineCNN parameters: {count_params(baseline_model):,}\")\n",
    "print(f\"LeNetLikeCNN parameters: {count_params(lenet_model):,}\")\n",
    "print(f\"AlexNetMini parameters: {count_params(alexmini_model):,}\")\n",
    "\n",
    "# Define common loss and optimizer settings\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train Baseline\n",
    "print(\"\\n= Training Baseline CNN (we are using pre-trainned models) =\")\n",
    "# Train LeNet-like\n",
    "print(\"\\n= Training LeNet-like CNN (we are using pre-trainned models) =\")\n",
    "# Train AlexNet-mini\n",
    "print(\"\\n= Training AlexNet-mini CNN (we are using pre-trainned models) =\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e5e41",
   "metadata": {},
   "source": [
    "**Parameter counts:**\n",
    "  - Baseline: ~4.2M parameters\n",
    "  - LeNet-like: ~4.2M (similar due to small extra conv layer)\n",
    "  - AlexNet-mini: ~2.6M (deeper but uses smaller FC layers)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca876a8c-fc5a-4e8b-920c-11edf879242a",
   "metadata": {},
   "source": [
    "**✅ ALERT: You are now ready to answer the Multiple Choice Questions for this section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5c29a-8c51-4a82-ace0-7a7830eae5f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb1a6f9-672f-47ac-944b-044e6c0fcb39",
   "metadata": {},
   "source": [
    "**✅ Code Task 5.4.5.1: Train for One Epoch (Quick Loop on Small Split)**\n",
    "\n",
    "📘 Instruction</br>\n",
    "Implement a minimal one-epoch training function for CPU:\n",
    "- Name: CT_train_one_epoch(model, loader, criterion, optimizer, device='cpu') → (avg_loss, avg_acc)\n",
    "- Accuracy: proportion of correct argmax predictions.\n",
    "- Train only 1 epoch on CT_train_loader using CT_model.\n",
    "- Use criterion = nn.CrossEntropyLoss() and optimizer = torch.optim.Adam(CT_model.parameters(), lr=1e-3) (or similar).\n",
    "</br>Store: CT_train_loss1, CT_train_acc1 (floats) and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c170d6bb-73b6-4252-b508-6141a68043e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT_train_loss1: 3.973512720802556\n",
      "CT_train_acc1: 0.02751358695652174\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "CT_criterion = nn.CrossEntropyLoss()\n",
    "CT_optimizer = torch.optim.Adam(CT_model.parameters(), lr=1e-3)\n",
    "\n",
    "def CT_train_one_epoch(model, loader, criterion, optimizer, device='cpu'):\n",
    "    model.train()\n",
    "    running_loss, running_corrects, total = 0.0, 0, 0\n",
    "    for CT_inputs, CT_labels in loader:\n",
    "        CT_inputs, CT_labels = CT_inputs.to(device), CT_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        CT_outputs = model(CT_inputs)\n",
    "        CT_loss = criterion(CT_outputs, CT_labels)\n",
    "        CT_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += CT_loss.item() * CT_inputs.size(0)\n",
    "        running_corrects += (CT_outputs.argmax(1) == CT_labels).sum().item()\n",
    "        total += CT_labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss /total\n",
    "    avg_acc  = running_corrects /total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "CT_train_loss1, CT_train_acc1 = CT_train_one_epoch(CT_model, CT_train_loader, CT_criterion, CT_optimizer, device=device)\n",
    "print(\"CT_train_loss1:\", CT_train_loss1)\n",
    "print(\"CT_train_acc1:\", CT_train_acc1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb7bd32e-5fb7-40f7-a1d5-2f341b07eeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT_train_loss1: 3.612013155999391\n",
      "CT_train_acc1: 0.026834239130434784\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "CT_criterion = nn.CrossEntropyLoss()\n",
    "CT_optimizer = torch.optim.Adam(CT_model.parameters(), lr=1e-3)\n",
    "\n",
    "def CT_train_one_epoch(model, loader, criterion, optimizer, device='cpu'):\n",
    "    model.train()\n",
    "    running_loss, running_corrects, total = 0.0, 0, 0\n",
    "    for CT_inputs, CT_labels in loader:\n",
    "        CT_inputs, CT_labels = CT_inputs.to(device), CT_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        CT_outputs = model(CT_inputs)\n",
    "        CT_loss = criterion(CT_outputs, CT_labels)\n",
    "        CT_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += CT_loss.item() * CT_inputs.size(0)\n",
    "        running_corrects += (CT_outputs.argmax(1) == CT_labels).sum().item()\n",
    "        total += CT_labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    avg_acc  = running_corrects / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "CT_train_loss1, CT_train_acc1 = CT_train_one_epoch(CT_model, CT_train_loader, CT_criterion, CT_optimizer, device=device)\n",
    "print(\"CT_train_loss1:\", CT_train_loss1)\n",
    "print(\"CT_train_acc1:\", CT_train_acc1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540bccaf-b192-4fa2-8759-23ab27cf92ca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab73864",
   "metadata": {},
   "source": [
    "### **6. Evaluation & Comparison**\n",
    "\n",
    "We now evaluate and compare **all three trained architectures** on the **held-out test set**:\n",
    "\n",
    "- **Baseline CNN (2 conv layers)**\n",
    "- **LeNet-like CNN (3 conv layers)**\n",
    "- **AlexNet-mini (5 conv layers)**\n",
    "\n",
    "**Metrics for Comparison**\n",
    "\n",
    "1. **Overall Test Accuracy**  \n",
    "   - Quick benchmark for generalization.\n",
    "\n",
    "2. **Parameter Count vs Accuracy**  \n",
    "   - Does adding depth lead to proportional gains?  \n",
    "   - Are deeper models *always* better?\n",
    "\n",
    "Let’s compute these metrics for side-by-side comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ce6b0",
   "metadata": {},
   "source": [
    "**Evaluation Function & Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd38b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reuse helper to compute accuracy\n",
    "def evaluate_model(model, loader, device='cpu'):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc\n",
    "\n",
    "# Evaluate all three models\n",
    "baseline_acc = evaluate_model(baseline_model, test_loader)\n",
    "lenet_acc = evaluate_model(lenet_model, test_loader)\n",
    "alexmini_acc = evaluate_model(alexmini_model, test_loader)\n",
    "\n",
    "print(f\"Baseline CNN Test Accuracy: {baseline_acc:.4f}\")\n",
    "print(f\"LeNet-like CNN Test Accuracy: {lenet_acc:.4f}\")\n",
    "print(f\"AlexNet-mini CNN Test Accuracy: {alexmini_acc:.4f}\")\n",
    "\n",
    "# Parameter counts\n",
    "baseline_params = count_params(baseline_model)\n",
    "lenet_params = count_params(lenet_model)\n",
    "alexmini_params = count_params(alexmini_model)\n",
    "\n",
    "# Compare parameter count vs accuracy\n",
    "print(\"\\nParameter vs Accuracy Comparison:\")\n",
    "print(f\"Baseline: {baseline_params:,} params → {baseline_acc:.2%}\")\n",
    "print(f\"LeNet-like: {lenet_params:,} params → {lenet_acc:.2%}\")\n",
    "print(f\"AlexNet-mini: {alexmini_params:,} params → {alexmini_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad11c5e",
   "metadata": {},
   "source": [
    "#### **Reflection**\n",
    "\n",
    "- **Overall Accuracy:**  \n",
    "  - Baseline and LeNet-like perform similarly (~10–15%).  \n",
    "  - AlexNet-mini struggles — deeper ≠ better when data is limited.\n",
    "\n",
    "- **Parameter vs Accuracy Trade-off:**  \n",
    "  - Despite having more layers, AlexNet-mini has *fewer* parameters (smaller FC layers) but still performs worse.  \n",
    "  - LeNet-like adds depth without significant parameter penalty, slightly better generalization.\n",
    "\n",
    "- **Learning Dynamics:**  \n",
    "  - Training curves show **faster overfitting** in deeper networks.\n",
    "  - Validation accuracy plateaued early — regularization (e.g., dropout, augmentation) is needed.\n",
    "\n",
    "This sets the stage for **Section 7**, where we reflect conceptually on **depth vs generalization** and link findings to real-world architecture choices (LeNet, AlexNet, VGG).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402abe77-1b4f-43db-8872-21ab5260fadf",
   "metadata": {},
   "source": [
    "**✅ ALERT: You are now ready to answer the Multiple Choice Questions for this section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ae89e-96a8-47d6-abea-b771e0c4580f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4528171f-3908-4682-98c9-dedfe9c04f0c",
   "metadata": {},
   "source": [
    "**✅ Code Task 5.4.6.1: Quick Validation + Baseline vs Variant Comparison**\n",
    "\n",
    "📘 Instruction</br>\n",
    "Implement a simple evaluation function and compare BaselineCNN (no extra training here) vs CT_LeNetVariant (after your 1-epoch train).\n",
    "- Write CT_evaluate(model, loader, device='cpu') → float_accuracy.\n",
    "- Compute CT_val_acc_baseline and CT_val_acc_variant on CT_val_loader.\n",
    "- Also compute CT_params_baseline and CT_params_variant using your CT_count_params.\n",
    "- Store results in a dict CT_compare = {...} and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a714c7a7-46f0-45bb-8597-fb593290845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT_compare: {'baseline_acc': 0.019021739130434784, 'variant_acc': 0.035326086956521736, 'params_baseline': 4204293, 'params_variant': 0.035326086956521736}\n"
     ]
    }
   ],
   "source": [
    "def CT_evaluate(model, loader, device='cpu'):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for CT_imgs, CT_lbls in loader:\n",
    "            CT_imgs, CT_lbls = CT_imgs.to(device), CT_lbls.to(device)\n",
    "            CT_out = model(CT_imgs)\n",
    "            CT_pred = CT_out.argmax(1)\n",
    "            correct += (CT_pred == CT_lbls).sum().item()\n",
    "            total += CT_lbls.size(0)\n",
    "    return correct / total\n",
    "\n",
    "CT_baseline_for_eval = BaselineCNN().to(device)   # fresh baseline\n",
    "CT_params_baseline = CT_count_params(CT_baseline_for_eval)\n",
    "CT_params_variant  = CT_count_params(CT_model)\n",
    "\n",
    "CT_val_acc_baseline = CT_evaluate(CT_baseline_for_eval, CT_val_loader, device=device)\n",
    "CT_val_acc_variant  = CT_evaluate(CT_model, CT_val_loader, device=device)\n",
    "\n",
    "CT_compare = {\n",
    "    \"baseline_acc\": CT_val_acc_baseline ,\n",
    "    \"variant_acc\":  CT_val_acc_variant,\n",
    "    \"params_baseline\": CT_params_baseline,\n",
    "    \"params_variant\": CT_val_acc_variant\n",
    "}\n",
    "print(\"CT_compare:\", CT_compare)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13dca3-a4da-4c5a-a90c-ed506cdc9836",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80d1c8",
   "metadata": {},
   "source": [
    "### **7. Reflection on Depth vs Generalization**\n",
    "\n",
    "#### **Feature Hierarchy in CNNs**\n",
    "\n",
    "- **Shallow CNNs (Baseline):**\n",
    "  - First conv layer detects **edges and simple colors**.\n",
    "  - Second layer combines edges into **basic textures** (e.g., fur patterns).\n",
    "\n",
    "- **Deeper CNNs (LeNet-like, AlexNet-mini):**\n",
    "  - Additional layers can capture **mid- and high-level features**:\n",
    "    - **Edges → textures → object parts → full objects**\n",
    "  - Theoretically better for **fine-grained classification** (e.g., 37 pet breeds).\n",
    "\n",
    "  \n",
    "**Does Deeper Always Mean Better?**\n",
    "\n",
    "- **Our Results:**\n",
    "  - LeNet-like slightly outperformed baseline (modest depth gain).\n",
    "  - AlexNet-mini **did not improve** despite more layers:\n",
    "    - Likely due to **limited training data** and **no augmentation**.\n",
    "    - Deeper models require **more data** to learn effectively.\n",
    "\n",
    "- **Overfitting Risk:**\n",
    "  - Deeper models memorized training data quickly (high train accuracy).\n",
    "  - Validation accuracy plateaued early → classic sign of **overfitting**.\n",
    "\n",
    "**Parameter Efficiency vs Performance**\n",
    "\n",
    "- Baseline vs LeNet-like: Similar parameter counts, slight accuracy boost.\n",
    "- AlexNet-mini: More layers, **fewer fully connected parameters**, but weaker results → depth alone isn’t sufficient.\n",
    "\n",
    "**Practical Considerations**\n",
    "\n",
    "- **Training Cost:**  \n",
    "  - More layers = more computation (even on CPU-only setup).  \n",
    "  - Small gains may not justify cost unless dataset size increases.\n",
    "\n",
    "- **Data Augmentation (NB03 Insight):**  \n",
    "  - Crucial for enabling deeper models to generalize without overfitting.\n",
    "\n",
    "- **Transfer Learning (Future Project):**  \n",
    "  - Pretrained deeper networks (ResNet, VGG) can leverage learned features and outperform shallow models on small datasets.\n",
    "\n",
    "**Takeaway**\n",
    "\n",
    "Depth is **powerful but not magical**:  \n",
    "- Without **enough data or regularization**, deeper CNNs fail to generalize.  \n",
    "- Next steps (future notebooks) involve **augmentation + transfer learning** to unlock the full potential of deep architectures.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
