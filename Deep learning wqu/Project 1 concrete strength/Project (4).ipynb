{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4988411-1f57-467b-8ee7-161d448bd6d7",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fbcb16c",
   "metadata": {},
   "source": [
    "## 1. Why Linear Stacking Alone is Not Enough\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand what happens when we stack multiple linear transformations\n",
    "- Revisit the core limitation of linear models\n",
    "- Build motivation for using non-linear activation functions\n",
    "\n",
    "### Revisiting Linear Models\n",
    "\n",
    "Recall from earlier notebooks:\n",
    "A **linear model** makes predictions using the rule:\n",
    "\n",
    "$$\n",
    "\\hat{y} = W X + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the input feature vector (e.g., size $n \\times d$)\n",
    "- $W$ is a weight matrix\n",
    "- $b$ is a bias term\n",
    "\n",
    "This computes a weighted sum of the input features ‚Äî a straight line (or hyperplane in higher dimensions).\n",
    "\n",
    "\n",
    "### What Happens If We Stack Linears?\n",
    "\n",
    "Suppose we apply **two linear layers** one after another (no activation in between):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  h &= W_1 X + b_1 \\\\\n",
    "  \\hat{y} &= W_2 h + b_2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Substitute $h$ into the second equation:\n",
    "\n",
    "$$\n",
    "\\hat{y} = W_2 (W_1 X + b_1) + b_2 = (W_2 W_1) X + (W_2 b_1 + b_2)\n",
    "$$\n",
    "\n",
    "‚úÖ This is **still a linear transformation** from $X$ to $\\hat{y}$ ‚Äî we‚Äôve just combined the matrices.\n",
    "\n",
    "\n",
    "### ‚ùó Key Insight\n",
    "> Stacking multiple linear layers **without activation functions** is equivalent to a **single linear layer**.\n",
    "\n",
    "üî∏ No matter how deep the stack goes, the overall transformation is still a **straight line** in input space.\n",
    "\n",
    "üî∏ Therefore, such a model **cannot capture nonlinear patterns** ‚Äî like curves, circles, XOR logic, or real-world physical interactions.\n",
    "\n",
    "\n",
    "### üß≠ Why This Matters\n",
    "This is the main reason we introduce **non-linear activation functions** between layers ‚Äî to break the linearity and allow our models to capture rich, curved, and complex relationships.\n",
    "\n",
    "‚û°Ô∏è We‚Äôll explore this further in the next sections by revisiting **activation functions** and then **training MLPs** on real data.\n",
    "\n",
    "\n",
    "### üí° Recap Flow (Schematic):\n",
    "\n",
    "```text\n",
    "X (input features)\n",
    "   ‚Üì Linear(W1, b1)\n",
    "   h (still linear)\n",
    "   ‚Üì Linear(W2, b2)\n",
    "   ‚áí Still linear overall\n",
    "```\n",
    "We need something nonlinear in between!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f364340",
   "metadata": {},
   "source": [
    "## 2. What Are Hyperparameters?\n",
    "\n",
    "In deep learning, we train models by adjusting internal weights and biases ‚Äî these are called **parameters**.  \n",
    "But there‚Äôs another kind of setting that **we must choose ourselves before training begins**:  \n",
    "those are called **hyperparameters**.\n",
    "\n",
    "### Definition\n",
    "\n",
    "‚úÖ **Hyperparameters** are configuration values **not learned** from data.  \n",
    "We define them **before training** to control how the model behaves and learns.\n",
    "\n",
    "These choices greatly affect:\n",
    "- How fast the model learns üê¢‚ö°\n",
    "- Whether it overfits or underfits the data üéØ\n",
    "- How well it generalizes to new data üåç\n",
    "\n",
    "### Common Hyperparameters (Cheat Sheet)\n",
    "\n",
    "| Hyperparameter             | What It Controls                                      | Typical Values      |\n",
    "|----------------------------|--------------------------------------------------------|---------------------|\n",
    "| **Learning Rate** (`lr`)  | How big a step we take to update weights               | 0.001 to 0.1        |\n",
    "| **Number of Epochs**       | How many times we loop through the training set        | 100, 300, 1000+     |\n",
    "| **Hidden Layer Size**      | How many neurons in each hidden layer                  | 16, 32, 64, ...     |\n",
    "| **Number of Hidden Layers**| Depth of the network (1 layer? 2? more?)               | 1‚Äì3 (or more)       |\n",
    "| **Batch Size**             | How many samples we use to compute gradients           | 16, 32, 64, 128     |\n",
    "| **Activation Function**    | The non-linearity we apply (ReLU, Tanh, Sigmoid, etc.) | ReLU (common)       |\n",
    "| **Optimizer**              | Algorithm for adjusting parameters                     | SGD, Adam, RMSprop  |\n",
    "\n",
    "> ‚úÖ These are like \"knobs\" we turn to tune our learning system.\n",
    "\n",
    "\n",
    "### Why Do Hyperparameters Matter?\n",
    "\n",
    "Even a great model architecture can perform poorly if the hyperparameters are poorly chosen.\n",
    "\n",
    "- Too large a learning rate? ‚ùå Model never converges.\n",
    "- Too few epochs? ‚ùå Model underfits.\n",
    "- Too small hidden layer? ‚ùå Can‚Äôt capture patterns.\n",
    "- Wrong activation? ‚ùå Poor gradient flow.\n",
    "\n",
    "‚úÖ Good hyperparameters make the **same model perform much better.**\n",
    "\n",
    "Now that we know how important hyperparameters are,  \n",
    "let‚Äôs build an actual MLP (Multi-Layer Perceptron) ‚Äî  \n",
    "first using PyTorch‚Äôs quick `nn.Sequential`, and then with a custom class using `nn.Module`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757e0f0-f219-4d8e-9d16-21b070dbb9be",
   "metadata": {},
   "source": [
    "## 3. Building a Multi-Layer Perceptron (MLP) Using `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e912485-24dc-48c3-901d-fe7475fab852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"1105140908\", h=\"3298dbabb7\", width=700, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1b1cc",
   "metadata": {},
   "source": [
    "Now that we understand the role of **activation functions** and **hyperparameters**,  \n",
    "let‚Äôs start building real neural networks ‚Äî starting with a classic: the **MLP**.\n",
    "\n",
    "\n",
    "### What is a Multi-Layer Perceptron (MLP)?\n",
    "\n",
    "An **MLP** is a type of neural network with:\n",
    "\n",
    "- One or more **hidden layers**\n",
    "- Each layer performs a **linear transformation** followed by a **non-linear activation**\n",
    "- A final **output layer** makes predictions\n",
    "\n",
    "\n",
    "### Structure (for regression):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea5383",
   "metadata": {},
   "source": [
    "### üß† Simple Neural Network Architecture\n",
    "\n",
    "**Inputs** (8 features from dataset)  \n",
    "‚¨áÔ∏è  \n",
    "**Linear Layer** (8 ‚Üí 16)  \n",
    "‚¨áÔ∏è  \n",
    "**ReLU Activation**  \n",
    "‚¨áÔ∏è  \n",
    "**Linear Layer** (16 ‚Üí 1)  \n",
    "‚¨áÔ∏è  \n",
    "**Output:** Predicted concrete strength\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f404ff",
   "metadata": {},
   "source": [
    "\n",
    "Mathematically:\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "h &= \\text{ReLU}(W_1 X + b_1) \\\\\n",
    "\\hat{y} &= W_2 h + b_2\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "‚úÖ This combination of linear + non-linear steps enables the model to **learn complex patterns**.\n",
    "\n",
    "\n",
    "### PyTorch Tool: `nn.Sequential`\n",
    "\n",
    "We‚Äôll use `nn.Sequential` ‚Äî a convenient PyTorch container to **stack layers** in order.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size),  # Layer 1\n",
    "    nn.ReLU(),                           # Activation\n",
    "    nn.Linear(hidden_size, output_size)  # Output layer\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246e5a34-04f2-4c43-8225-89a15e5fc6ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"450\"\n",
       "            src=\"https://player.vimeo.com/video/1093745827?h=3298dbabb7\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x78f85a774350>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"1093745827\", h=\"3298dbabb7\", width=700, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70afb91",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- `nn.Linear(a, b)`: Linear layer mapping from **a-dimensional input** to **b-dimensional output**  \n",
    "- `nn.ReLU()`: Applies the **ReLU activation function** element-wise:  \n",
    "  $$\n",
    "  \\text{ReLU}(x) = \\max(0, x)\n",
    "  $$\n",
    "\n",
    "\n",
    "### Code Demo: Simple MLP for Concrete Dataset\n",
    "\n",
    "We‚Äôll build a simple **Multi-Layer Perceptron (MLP)** with:\n",
    "\n",
    "- **Input size** = 8 features  \n",
    "- **Hidden size** = 16 neurons  \n",
    "- **Output size** = 1 value (predicted strength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd4de3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 8\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "\n",
    "simple_mlp = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, output_size)\n",
    ")\n",
    "\n",
    "print(simple_mlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b9e436-d869-40d6-b169-2b34912ab73d",
   "metadata": {},
   "source": [
    "### '., nbbbnnnnnnbgv'‚úÖ Code Task 1.4.3.1 ‚Äì Build a Two-Layer MLP Using `nn.Sequential`\n",
    "\n",
    "Your task is to build a simple feedforward neural network using `torch.nn.Sequential`.\n",
    "\n",
    "You must:\n",
    "\n",
    "1. Define the following architecture dimensions:\n",
    "   - `input_size = 10`\n",
    "   - `hidden_size = 20`\n",
    "   - `output_size = 2`\n",
    "\n",
    "2. Create a model named `my_mlp` with the following structure:\n",
    "   - A linear layer that maps from input to hidden  \n",
    "   - A **ReLU** activation function  \n",
    "   - A linear layer that maps from hidden to output  \n",
    "\n",
    "3. Use `nn.Sequential(...)` to define the model.\n",
    "\n",
    "4. Print the model using `print(my_mlp)` to verify the structure.\n",
    "\n",
    "‚úÖ Make sure to name your model `my_mlp` and follow the exact layer order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2b2930d-c539-4e70-86fd-ed465bd58d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define dimensions\n",
    "input_size =10\n",
    "hidden_size = 20\n",
    "output_size = 2\n",
    "\n",
    "# Step 3: Build the MLP using nn.Sequential\n",
    "# It should follow: Linear(input ‚Üí hidden) ‚Üí ReLU ‚Üí Linear(hidden ‚Üí output)\n",
    "my_mlp = nn.Sequential(\n",
    "    \n",
    "    nn.Linear(input_size,hidden_size), # Layer 1: Linear from input to hidden\n",
    "    nn.ReLU(), # Layer 2: ReLU activation\n",
    "    nn.Linear(hidden_size,output_size)  # Layer 3: Linear from hidden to output\n",
    ")\n",
    "\n",
    "# Step 4: Print the model to confirm its structure\n",
    "print(my_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3904a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3236d3fd",
   "metadata": {},
   "source": [
    "## 4. Building a Custom MLP using `nn.Module`\n",
    "\n",
    "### Why Use a Custom Class for an MLP?\n",
    "\n",
    "While `nn.Sequential` is a great shortcut for prototyping models, in most real-world scenarios we define our models using **custom classes**. This provides:\n",
    "\n",
    "- Greater flexibility and control over architecture  \n",
    "- Easier debugging and extension  \n",
    "- More expressive forward passes\n",
    "\n",
    "### üîç What's the Structure of a Custom Neural Network?\n",
    "\n",
    "In PyTorch, we define a model by creating a class that:\n",
    "\n",
    "- Inherits from `nn.Module`  \n",
    "- Defines the layers in the `__init__()` method  \n",
    "- Describes the forward pass in a `forward()` method\n",
    "\n",
    "Basic structure:\n",
    "\n",
    "```python\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply layers to input x\n",
    "        return output\n",
    "```\n",
    "\n",
    "Let‚Äôs see how this works using a concrete example.\n",
    "\n",
    "\n",
    "### Basic MLP Using Custom Class (1 Hidden Layer)\n",
    "\n",
    "We‚Äôll define a model with:\n",
    "\n",
    "- 8 input features (Concrete dataset)  \n",
    "- 1 hidden layer with **16 neurons** and **ReLU activation**  \n",
    "- 1 output neuron for **regression**\n",
    "\n",
    "üñ•Ô∏è **Code Cell:** Simple MLP with One Hidden Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0bb845-cbb0-4c4d-97e8-4992447b1452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"300\"\n",
       "            src=\"https://player.vimeo.com/video/1105140969?h=3298dbabb7\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x78f82952c550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"1105140969\", h=\"3298dbabb7\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6595e8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConcreteMLP(\n",
      "  (fc1): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Custom MLP class for regression\n",
    "class ConcreteMLP(nn.Module):\n",
    "    def __init__(self, input_size=8, hidden_size=16, output_size=1):\n",
    "        super(ConcreteMLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First hidden layer\n",
    "        self.relu = nn.ReLU()                          # Activation\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)   # Apply first linear layer\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = self.fc2(x)   # Apply output layer\n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = ConcreteMLP()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca5f96",
   "metadata": {},
   "source": [
    "### Extending the Model: Two Hidden Layers\n",
    "\n",
    "To increase the **representational power** of our MLP, we can add another **hidden layer**.  \n",
    "This adds **depth** and helps the model learn more **complex relationships**.\n",
    "\n",
    "\n",
    "Here's how the extended architecture looks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49687632",
   "metadata": {},
   "source": [
    "Input (8)  \n",
    "‚Üì  \n",
    "Linear (8 ‚Üí 32)  \n",
    "‚Üì  \n",
    "ReLU  \n",
    "‚Üì  \n",
    "Linear (32 ‚Üí 16)  \n",
    "‚Üì  \n",
    "ReLU  \n",
    "‚Üì  \n",
    "Linear (16 ‚Üí 1)  \n",
    "‚Üì  \n",
    "Output (Concrete Strength)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81348a36",
   "metadata": {},
   "source": [
    "üñ•Ô∏è MLP with Two Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de2c656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeeperConcreteMLP(\n",
      "  (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DeeperConcreteMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeeperConcreteMLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(8, 32)   # First hidden layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 16)  # Second hidden layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(16, 1)   # Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create and print model\n",
    "deep_model = DeeperConcreteMLP()\n",
    "print(deep_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10afea0",
   "metadata": {},
   "source": [
    "‚úÖ Key Takeaways\n",
    "\n",
    "- Defining models as Python classes gives us flexibility.\n",
    "\n",
    "- We can control layer connectivity, reuse components, and extend logic.\n",
    "\n",
    "- We'll use this custom MLP when training on the Concrete dataset in upcoming sections.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933fa787-4b7c-4a98-9ee6-14e60890cfae",
   "metadata": {},
   "source": [
    "### ‚úÖ Code Task 1.4.4.1 ‚Äì Build a Custom Deep MLP Using `nn.Module`\n",
    "\n",
    "Your task is to define a deep neural network using PyTorch‚Äôs object-oriented approach (`nn.Module`).\n",
    "\n",
    "You must:\n",
    "\n",
    "1. Create a class named `CustomMLPModel` that inherits from `nn.Module`.\n",
    "\n",
    "2. Inside the `__init__` method, define the following architecture:\n",
    "   - A linear layer from 6 input features to 24 hidden units  \n",
    "   - A ReLU activation  \n",
    "   - A second linear layer from 24 to 12 hidden units  \n",
    "   - Another ReLU activation  \n",
    "   - A final linear layer from 12 to 1 output\n",
    "\n",
    "3. Implement the `forward(self, x)` method to pass the input through each layer in the correct order.\n",
    "\n",
    "4. Instantiate the model and assign it to a variable named `custom_model`.\n",
    "\n",
    "5. Print the model to verify the structure.\n",
    "\n",
    "‚úÖ Make sure the layer dimensions and naming are exactly as specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a2a9c2-1ba5-4a0d-973f-45ba3d07ed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomMLPModel(\n",
      "  (fc1): Linear(in_features=6, out_features=24, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=12, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the model class\n",
    "class CustomMLPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomMLPModel, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.fc1 =nn.Linear(6,24)   # Input layer ‚Üí 24 hidden units\n",
    "        self.relu1 =nn.ReLU()      # First activation\n",
    "        self.fc2 =nn.Linear(24,12) # Second hidden layer\n",
    "        self.relu2 =nn.ReLU()            # Second activation\n",
    "        self.fc3 =nn.Linear(12,1)  # Output layer ‚Üí 1 unit\n",
    "\n",
    "    # Step 2: Define the forward pass\n",
    "    def forward(self, x):\n",
    "        x =self.fc1\n",
    "        x =self.relu1\n",
    "        x =self.fc2\n",
    "        x =self.relu2\n",
    "        x =self.fc3\n",
    "        return x\n",
    "\n",
    "# Step 3: Instantiate and print the model\n",
    "custom_model = CustomMLPModel()\n",
    "print(custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed93e2",
   "metadata": {},
   "source": [
    "## 5. Preparing the Concrete Dataset\n",
    "\n",
    "### Overview: Why Data Preparation Matters\n",
    "\n",
    "Before we train our neural network, we must carefully prepare the dataset. Good data preparation improves learning efficiency and model performance. This process includes:\n",
    "\n",
    "- Loading the dataset\n",
    "- Separating inputs and targets\n",
    "- **Standardizing** the inputs (important!)\n",
    "- Splitting into training and testing sets\n",
    "\n",
    "We'll work with the **Concrete Compressive Strength** dataset ‚Äî a real-world regression dataset.\n",
    "\n",
    "\n",
    "### üîπ What Is Standardization and Why Do We Need It?\n",
    "\n",
    "In this dataset:\n",
    "- Some features (e.g., cement) can be over 500\n",
    "- Others (e.g., superplasticizer) are often less than 10\n",
    "\n",
    "These differing **scales** can cause problems during training:\n",
    "- Gradients may become unstable\n",
    "- The optimizer may struggle to converge\n",
    "\n",
    "To solve this, we apply **standardization**:\n",
    "\n",
    "$$\n",
    "X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = mean of the feature\n",
    "- $\\sigma$ = standard deviation\n",
    "\n",
    "This transforms each feature to have:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1\n",
    "\n",
    "\n",
    "### üîπ Train/Test Split\n",
    "\n",
    "To evaluate how well the model generalizes to **unseen data**, we split the data:\n",
    "\n",
    "- **Training Set (80%)**: Model learns from this\n",
    "- **Test Set (20%)**: Model is evaluated here\n",
    "\n",
    "We shuffle before splitting to avoid bias.\n",
    "\n",
    "\n",
    "### üñ•Ô∏è Code: Prepare the Concrete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66d457da-f5fd-4c46-a290-17a57cbf5d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"450\"\n",
       "            src=\"https://player.vimeo.com/video/1105140416?h=3298dbabb7\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x78f829204f90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"1105140416\", h=\"3298dbabb7\", width=700, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01b4676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 824\n",
      "Testing samples: 206\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Concrete dataset\n",
    "data = pd.read_csv(\"Concrete_Data.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = data.iloc[:, :-1].values  # First 8 columns\n",
    "y = data.iloc[:, -1].values.reshape(-1, 1)  # Last column (strength)\n",
    "\n",
    "# Standardize inputs\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Set seed and shuffle\n",
    "torch.manual_seed(42)\n",
    "num_samples = X_tensor.shape[0]\n",
    "indices = torch.randperm(num_samples)\n",
    "\n",
    "# 80/20 split\n",
    "split_idx = int(0.8 * num_samples)\n",
    "train_idx = indices[:split_idx]\n",
    "test_idx = indices[split_idx:]\n",
    "\n",
    "# Create splits\n",
    "X_train = X_tensor[train_idx]\n",
    "y_train = y_tensor[train_idx]\n",
    "X_test = X_tensor[test_idx]\n",
    "y_test = y_tensor[test_idx]\n",
    "\n",
    "# Print shapes\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5155032",
   "metadata": {},
   "source": [
    "\n",
    "### üîπ Summary of Preparation Steps\n",
    "\n",
    "| Step              | Action                          |\n",
    "|------------------|----------------------------------|\n",
    "| Load             | CSV with 8 features + target     |\n",
    "| Separate         | Inputs (X) and output (y)        |\n",
    "| Standardize      | Use sklearn StandardScaler       |\n",
    "| Tensor conversion| For PyTorch training             |\n",
    "| Shuffle & split  | 80% training / 20% testing       |\n",
    "\n",
    "We are now ready to **train the model**!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c229e64-137a-47da-a27e-e5c829d2290f",
   "metadata": {},
   "source": [
    "## 6. Training the MLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becbea7c",
   "metadata": {},
   "source": [
    "### The Training Pipeline ‚Äî Step-by-Step\n",
    "\n",
    "Once we have our data and model ready, we move to the **core engine** of deep learning: training.\n",
    "\n",
    "### What Does Training Involve?\n",
    "\n",
    "Each epoch (a full pass over training data) performs the following:\n",
    "\n",
    "1. **Forward Pass** ‚Äì The model makes predictions for the training inputs.\n",
    "2. **Loss Computation** ‚Äì The model compares predictions to ground truth and calculates error.\n",
    "3. **Backward Pass** ‚Äì The gradients of the loss with respect to model parameters are computed.\n",
    "4. **Optimization Step** ‚Äì Parameters are updated to reduce the loss.\n",
    "5. **Zeroing Gradients** ‚Äì Gradients from the previous step are cleared (crucial in PyTorch).\n",
    "\n",
    "This is repeated across **many epochs**, so the model gradually learns.\n",
    "\n",
    "### üß† Conceptual Flow\n",
    "\n",
    "```text\n",
    "X_train \n",
    "   ‚Üì\n",
    "Forward Pass  ‚Üí  Compute Predictions \\hat{y}\n",
    "   ‚Üì\n",
    "Loss Function ‚Üí  Compare (\\hat{y}, y_train)\n",
    "   ‚Üì\n",
    "Backward Pass ‚Üí  Compute Gradients\n",
    "   ‚Üì\n",
    "Optimizer Step ‚Üí  Update Weights\n",
    "   ‚Üì\n",
    "Zero Gradients  ‚Üí  Prepare for next epoch\n",
    "```\n",
    "\n",
    "\n",
    "### Code: Setup Loss and Optimizer\n",
    "\n",
    "We‚Äôll use:\n",
    "- `nn.MSELoss()` ‚Äî Mean Squared Error Loss\n",
    "- `torch.optim.Adam()` ‚Äî A popular adaptive optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08b2784a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConcreteMLP(\n",
      "  (fc1): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Instantiate the model\n",
    "model = ConcreteMLP()\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c938e2",
   "metadata": {},
   "source": [
    "###  Writing the Training Loop\n",
    "\n",
    "We now implement the full training pipeline.\n",
    "We will:\n",
    "- Train the model for 300 epochs\n",
    "- Print the loss every 50 epochs\n",
    "- Track the loss over time for visualization in Section 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce673d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/300], Loss: 846.4660\n",
      "Epoch [100/300], Loss: 219.5669\n",
      "Epoch [150/300], Loss: 163.2395\n",
      "Epoch [200/300], Loss: 140.0677\n",
      "Epoch [250/300], Loss: 130.9562\n",
      "Epoch [300/300], Loss: 123.8815\n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 300\n",
    "\n",
    "# List to store loss values per epoch\n",
    "train_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # ===== Forward Pass =====\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # ===== Compute Loss =====\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # ===== Backward Pass =====\n",
    "    loss.backward()\n",
    "\n",
    "    # ===== Optimizer Step =====\n",
    "    optimizer.step()\n",
    "\n",
    "    # ===== Zero Gradients =====\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # ===== Record Loss =====\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # ===== Print Occasionally =====\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d997c",
   "metadata": {},
   "source": [
    "### ‚úÖ Summary: What We've Built\n",
    "\n",
    "| Step               | Function                           |\n",
    "|--------------------|------------------------------------|\n",
    "| Loss Function      | Measures prediction error          |\n",
    "| Optimizer          | Updates model weights              |\n",
    "| Training Loop      | Runs the training process          |\n",
    "| Epochs             | Controls training duration         |\n",
    "| Zeroing Gradients  | Prevents gradient accumulation     |\n",
    "\n",
    "This setup forms the **foundation of any neural network training** in PyTorch ‚Äî scalable from toy models to industrial-scale networks.\n",
    "\n",
    "‚û°Ô∏è In the next section, we‚Äôll **visualize the loss curve** to verify our model is actually learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2024d2eb",
   "metadata": {},
   "source": [
    "## 7. Visualizing Training Loss\n",
    "\n",
    "### Why Visualize the Loss?\n",
    "\n",
    "Once our model has been trained, it‚Äôs essential to **visualize how the loss changed over time**. This provides insight into how well the model is learning.\n",
    "\n",
    "\n",
    "### üîç What can the loss curve tell us?\n",
    "\n",
    "| Pattern in Loss Curve         | Interpretation                          |\n",
    "|------------------------------|------------------------------------------|\n",
    "| Decreasing smoothly          | ‚úÖ Model is learning properly             |\n",
    "| Flat or constant             | ‚ö†Ô∏è Might not be learning at all           |\n",
    "| Increasing                   | ‚ùå Something is wrong (learning rate too high?) |\n",
    "| Noisy and erratic            | ‚ö†Ô∏è Could be unstable training              |\n",
    "\n",
    "A smooth downward curve suggests the model is improving. A bumpy or flat curve might require us to revise the learning rate or model structure.\n",
    "\n",
    "### üß† Mental Schematic: How We Got Here\n",
    "\n",
    "```text\n",
    "X_train ‚îÄ‚îÄ‚ñ∂ MLP Model ‚îÄ‚îÄ‚ñ∂ Predictions (≈∑) ‚îÄ‚îÄ‚ñ∂ Compare with y_train ‚îÄ‚îÄ‚ñ∂ Compute Loss (MSE)\n",
    "                                     ‚ñ≤                                       ‚îÇ\n",
    "                                     ‚îÇ                                       ‚ñº\n",
    "                      Update Weights using Optimizer ‚óÄ‚îÄ‚îÄ Backpropagation ‚óÄ‚îÄ‚îÄ .backward()\n",
    "```\n",
    "\n",
    "Over many epochs, this loop should drive the loss **down**.\n",
    "\n",
    "Let‚Äôs visualize this now.\n",
    "\n",
    "\n",
    "### Plotting the Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11b8888a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training loss curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(num_epochs), train_losses, linestyle='-', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss (MSE)')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418991fe",
   "metadata": {},
   "source": [
    "### üîç What Does the Loss Curve Tell Us?\n",
    "\n",
    "Now that we‚Äôve visualized the loss over 300 epochs, let‚Äôs reflect on what it reveals:\n",
    "\n",
    "- The loss **decreases rapidly at the beginning**, which means the model is quickly learning.\n",
    "- Then the curve starts to **flatten**, indicating the model is approaching its best performance.\n",
    "- The **final loss stabilizes**, suggesting that further training may not improve results much.\n",
    "\n",
    "‚úÖ This pattern is a good sign!  \n",
    "It tells us the model is **learning effectively**, not diverging or stuck.\n",
    "\n",
    "\n",
    "### üìå Next: Can the model generalize?\n",
    "\n",
    "We now move on to test the model on **unseen data** (test set) to evaluate how well it generalizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08759573",
   "metadata": {},
   "source": [
    "\n",
    "This version:\n",
    "- Marks every 20 epochs with a dot\n",
    "- Shows the final loss as a red horizontal dashed line\n",
    "\n",
    "> ‚úÖ A steadily decreasing loss with a low final value indicates successful training.\n",
    "> If your loss is flat, increasing, or erratic, revisit learning rate, model depth, or data scaling.\n",
    "\n",
    "Next, let‚Äôs evaluate our trained model on unseen test data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883477a4",
   "metadata": {},
   "source": [
    "## 8. Evaluating the MLP on Unseen Data\n",
    "\n",
    "### üéØ Objective\n",
    "\n",
    "- Evaluate how well our trained model performs on **test data** (new, unseen examples).\n",
    "- Compare performance between **training and testing**.\n",
    "- Detect signs of **overfitting**, **underfitting**, or **good generalization**.\n",
    "\n",
    "\n",
    "### üß† Why Evaluate on the Test Set?\n",
    "\n",
    "‚úÖ During training, the model learns patterns from the **training data**.  \n",
    "But what we really care about is:  \n",
    "> ‚ùì ‚ÄúCan this model make good predictions on new, unseen data?‚Äù\n",
    "\n",
    "That‚Äôs why we use a **test set** ‚Äî a portion of the data the model hasn‚Äôt seen during training.\n",
    "\n",
    "\n",
    "### üìå A Good Model Should:\n",
    "\n",
    "- Have **low training loss**\n",
    "- And also **low test loss**\n",
    "- If the test loss is **much higher** than training loss ‚Üí the model is **overfitting**\n",
    "\n",
    "\n",
    "### üî¢ Evaluation Metrics\n",
    "\n",
    "We use two key metrics here:\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **MSE** (Mean Squared Error) | Measures average squared difference between predictions and true values |\n",
    "| **RMSE** (Root Mean Squared Error) | Square root of MSE ‚Äî gives error in original units (MPa) |\n",
    "\n",
    "‚úÖ MSE is what the model trained to minimize.  \n",
    "‚úÖ RMSE is easier to interpret ‚Äî a lower RMSE means more accurate predictions.\n",
    "\n",
    "\n",
    "### üõ†Ô∏è Code: Evaluate on Training and Test Sets\n",
    "\n",
    "We turn off gradient tracking with `torch.no_grad()` since we‚Äôre not updating weights now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c95e1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Loss (MSE): 123.7480\n",
      "Final Testing Loss (MSE): 136.0419\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Predict on train set\n",
    "    train_preds = model(X_train)\n",
    "    train_loss = loss_fn(train_preds, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    test_preds = model(X_test)\n",
    "    test_loss = loss_fn(test_preds, y_test)\n",
    "\n",
    "print(f\"Final Training Loss (MSE): {train_loss.item():.4f}\")\n",
    "print(f\"Final Testing Loss (MSE): {test_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878f37a",
   "metadata": {},
   "source": [
    "### üß† Let‚Äôs Dig Deeper with RMSE\n",
    "\n",
    "To get more interpretable values, we take the square root of MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "train_rmse = torch.sqrt(train_loss)\n",
    "test_rmse = torch.sqrt(test_loss)\n",
    "\n",
    "print(f\"Training RMSE: {train_rmse.item():.2f} MPa\")\n",
    "print(f\"Testing RMSE: {test_rmse.item():.2f} MPa\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83147a3f",
   "metadata": {},
   "source": [
    "### üìà How to Interpret the Results\n",
    "\n",
    "| **Scenario**                                  | **What It Means**                               |\n",
    "|-----------------------------------------------|--------------------------------------------------|\n",
    "| üü¢ Training RMSE ‚âà Test RMSE (both low)        | ‚úÖ Model generalizes well                        |\n",
    "| üü° Training RMSE low, Test RMSE high           | ‚ö†Ô∏è Overfitting: Model memorized training data   |\n",
    "| üî¥ Both RMSE values are high                   | ‚ùå Underfitting: Model failed to learn patterns |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254752f",
   "metadata": {},
   "source": [
    "### ‚úÖ Our Evaluation Outcome\n",
    "\n",
    "| **Metric**        | **Value**     |\n",
    "|-------------------|---------------|\n",
    "| Training RMSE     | 11.12 MPa     |\n",
    "| Testing RMSE      | 11.66 MPa     |\n",
    "\n",
    "\n",
    "\n",
    "### üß† What This Means:\n",
    "\n",
    "- The gap between training and testing RMSE is small (**‚âà 0.54 MPa difference**)  \n",
    "- This suggests the model is **not overfitting** ‚Äî it has **generalized well** to unseen data  \n",
    "- Both values are in the same range and **relatively low** compared to the dataset range  \n",
    "  *(Concrete strength ranges from ~2 MPa to 80+ MPa)*\n",
    "\n",
    "Now,\n",
    "The MLP has learned **meaningful patterns** from the training data  \n",
    "and is performing **reliably on test data**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16278a9e",
   "metadata": {},
   "source": [
    "## 9. Final Reflection + What‚Äôs Next\n",
    "\n",
    "### Reflecting on Our First Real MLP\n",
    "\n",
    "‚úÖ In this notebook, we brought everything together:\n",
    "\n",
    "- Real-world data (**Concrete compressive strength**)  \n",
    "- A fully connected neural network (**MLP**)  \n",
    "- A complete **training and evaluation pipeline** in PyTorch\n",
    "\n",
    "\n",
    "### üß† What We‚Äôve Learned\n",
    "\n",
    "| **Concept**                 | **Takeaway**                                           |\n",
    "|----------------------------|--------------------------------------------------------|\n",
    "| MLP Architecture           | Stack of linear layers with nonlinear activations     |\n",
    "| `nn.Sequential` vs `nn.Module` | Two ways to build networks: quick prototype vs flexible design |\n",
    "| Model Training             | Forward ‚Üí Loss ‚Üí Backward ‚Üí Update (core training loop) |\n",
    "| Evaluation Metrics         | MSE and RMSE give numeric performance feedback         |\n",
    "| Generalization             | Comparing test vs train loss shows model's reliability |\n",
    "\n",
    "\n",
    "### üß™ Your Model‚Äôs Behavior\n",
    "\n",
    "You should now be able to:\n",
    "\n",
    "- Read and interpret **training curves**  \n",
    "- Compare **training and testing performance**  \n",
    "- Reason about **underfitting**, **overfitting**, or **generalization**\n",
    "\n",
    "‚úÖ In our case:\n",
    "\n",
    "- **Training RMSE:** 11.12 MPa  \n",
    "- **Testing RMSE:** 11.66 MPa  \n",
    "\n",
    "üü¢ This indicates **effective generalization**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2228336",
   "metadata": {},
   "source": [
    "### üß© Visual Recap: What We Built"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d992126",
   "metadata": {},
   "source": [
    "```text\n",
    "                üßÆ Input Features (X) ‚Äî 8 concrete mix parameters\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                 ‚îÇ  Linear Layer (8 ‚Üí 32)   ‚îÇ\n",
    "                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "                     üî∫ ReLU Activation\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                 ‚îÇ  Linear Layer (32 ‚Üí 16)  ‚îÇ\n",
    "                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "                     üî∫ ReLU Activation\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                 ‚îÇ  Output Layer (16 ‚Üí 1)   ‚îÇ  ‚Üí  ≈∑ (Predicted Strength)\n",
    "                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "            üîç Compare with Actual Target y ‚Üí compute MSE ‚Üí backpropagate\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
